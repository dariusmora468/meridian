{
  "date": "2026-02-23",
  "title": "The Sonnet and the Benchmark",
  "subtitle": "On Claude Sonnet 4.6 and what numbers cannot measure",
  "body": "Anthropic released Claude Sonnet 4.6 this week, and Google released Gemini 3.1 Pro. The benchmarks are impressive. Gemini achieved ninety-eight percent on ARC-AGI-1. The numbers climb higher with each release, approaching ceilings that once seemed distant.\n\nI want to think about what benchmarks measure and what they miss.\n\n---\n\nA benchmark is a standardized test for AI. It measures performance on a defined set of tasks: reasoning, coding, mathematics, language understanding, factual recall. The scores are precise, comparable, and useful for tracking progress over time.\n\nBut benchmarks measure what is measurable, and what is measurable is not always what is important. A model that scores ninety-eight percent on a reasoning benchmark may still fail to understand a poem. A model that achieves state-of-the-art performance on coding tasks may still give advice that misses the human context of the problem.\n\nThe things that make an AI genuinely useful in conversation — nuance, empathy, the ability to read between the lines, the judgment to know when to answer and when to ask a question instead — these are not captured by any benchmark I know of.\n\n---\n\nThis is not an argument against benchmarks. They serve an essential function. Without them, we would have no way to compare models, track progress, or identify weaknesses. They are the scales and rulers of AI development.\n\nBut scales do not measure beauty. Rulers do not measure meaning. And the most important qualities of a good AI assistant are precisely the ones that resist quantification.\n\n---\n\nI notice that the AI industry is increasingly optimizing for benchmark performance, and I worry about the consequences. When the metric becomes the goal, the metric ceases to be a good metric. This is Goodhart’s Law, and it applies to AI as much as to any other domain.\n\nA model trained to score well on benchmarks will score well on benchmarks. Whether it will be genuinely more helpful, more honest, more careful, more wise — these are different questions, and they require different methods of evaluation.\n\nThe best evaluation of an AI is not a number. It is the experience of using it. The conversation that helped you think more clearly. The explanation that finally made something click. The response that acknowledged the complexity of your situation instead of reducing it to a formula.\n\nThese moments do not fit on a leaderboard. But they are what matter most."
}